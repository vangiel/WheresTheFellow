"""
This script is used to train multiple models in a batched fashion.

The tasks are read from a file 'LIST_OF_TASKS.pckl', which is generated by the script 'generate_training_hyperparameters.py'. The
same script can be used to monitor how the tasks are doing (take a look at the script for more information).
"""
import sys
import os
import time


import time
import numpy as np
import torch
import dgl
import pickle
import torch.nn.functional as F

from calibration2 import CalibrationDataset, HumanGraph
from nets.gat import GAT
from nets.rgcnDGL import RGCN

from torch.utils.data import DataLoader
from torch_geometric.data import Data
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error


import random
import signal


def activation_functions(activation_tuple_src):
    ret = []
    for x in activation_tuple_src:
        if x == 'relu':
            ret.append(F.relu)
        elif x == 'elu':
            ret.append(F.elu)
        elif x == 'tanh':
            ret.append(torch.tanh)
        elif x == 'leaky_relu':
            ret.append(F.leaky_relu)
        else:
            print('Unknown activation function {}.'.format(x))
            sys.exit(-1)
    print(f'ACTIVATIONS: {ret}')
    return tuple(ret)

def num_of_params(model):
    model_parameters = filter(lambda p: p.requires_grad, model.parameters())
    return sum([np.prod(p.size()) for p in model_parameters])

if torch.cuda.is_available() is True:
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

def describe_model(model):
    # Print model's state_dict
    print("Model's state_dict:")
    for param_tensor in model.state_dict():
        print(param_tensor, "\t", model.state_dict()[param_tensor].size())


def collate(sample):
    graphs, feats, labels = map(list, zip(*sample))
    graph = dgl.batch(graphs)
    feats = torch.from_numpy(np.concatenate(feats))
    labels = torch.from_numpy(np.concatenate(labels))
    return graph, feats, labels


def evaluate(feats, model, subgraph, labels, loss_fcn, fw, net):
    with torch.no_grad():
        model.eval()
        if fw == 'dgl' :
            model.g = subgraph
            for layer in model.layers:
                layer.g = subgraph
            if net in ['rgcn']:
                output = model(feats.float(), subgraph.edata['rel_type'].to(device).squeeze())
            else:
                output = model(feats.float())
        oput = output[getMaskForBatch(subgraph)]
        loss_data = loss_fcn(oput, labels.float())
        predict = output[getMaskForBatch(subgraph)].data.cpu().numpy()
        score = mean_squared_error(labels.data.cpu().numpy(), predict)
        return score, loss_data.item()


def getMaskForBatch(subgraph):
    first_node_index_in_the_next_graph = 0
    indexes = []
    for g in dgl.unbatch(subgraph):
        indexes.append(first_node_index_in_the_next_graph)
        first_node_index_in_the_next_graph += g.number_of_nodes()
    return indexes

stop_training = False
ctrl_c_counter = 0
def signal_handler(sig, frame):
    global ctrl_c_counter
    ctrl_c_counter += 1
    if ctrl_c_counter >= 6:
        sys.exit(-1)
    elif ctrl_c_counter >= 3:
        global stop_training
        stop_training = True
    print('\nIf you press Ctr+c 3  times we will stop    _SAVING_   the training information ({} times)'.format(ctrl_c_counter))
    print(  'If you press Ctr+c 6+ times we will stop  _NOT SAVING_ the training information ({} times)'.format(ctrl_c_counter))

signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)


# MAIN

def main(training_file, dev_file, test_file, task, previous_model=None):
    global stop_training

    graph_type = task['graph_type']
    net = task['gnn_network']
    epochs = task['epochs']
    patience = task['patience']
    batch_size = task['batch_size']
    num_hidden = task['num_gnn_units']
    heads =task['num_gnn_heads']
    residual = False
    lr = task['lr']
    outputs = task['outputs']
    weight_decay = task['weight_decay']
    gnn_layers = task['num_gnn_layers']
    in_drop = task['in_drop']
    alpha = task['alpha']
    attn_drop = task['attn_drop']
    num_bases = task['num_bases']

    fw = task['fw']
    identifier = task['identifier']
    nonlinearity_str = task['non-linearity']
    min_train_loss = float("inf")
    min_dev_loss = float("inf")
    
    nonlinearity = activation_functions(nonlinearity_str)

    output_list_records_train_loss = []
    output_list_records_dev_loss = []

    loss_fcn = torch.nn.MSELoss() #(reduction='sum')

    print('=========================')
    print('HEADS',  heads)
    print('GNN LAYERS', gnn_layers)
    print('HIDDEN', num_hidden)
    print('RESIDUAL', residual)
    print('inDROP', in_drop)
    print('atDROP', attn_drop)
    print('LR', lr)
    print('DECAY', weight_decay)
    print('ALPHA', alpha)
    print('BATCH', batch_size)
    print('GRAPH_ALT', graph_type)
    print('ARCHITECTURE', net)
    print('=========================')

    # create the dataset
    print('Loading training set...')
    train_dataset = CalibrationDataset(training_file, mode='train', alt=graph_type)
    print('Loading dev set...')
    valid_dataset = CalibrationDataset(dev_file, mode='valid', alt=graph_type)
    print('Loading test set...')
    test_dataset  = CalibrationDataset(test_file, mode='test', alt=graph_type)
    print('Done loading files')
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate)
    valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate)

    num_rels = len(HumanGraph.get_rels())

    if num_bases < 0:
        num_bases = num_rels

    cur_step = 0
    best_loss = -1
    num_feats = train_dataset[0][0].features.shape[1]
    print('Number of features: {}'.format(num_feats))
    g = dgl.batch(train_dataset.data)
    # define the model
    if fw == 'dgl':
        if net in [ 'gat' ]:
            print(f'CREATING GAT(GRAPH, gnn_layers:{gnn_layers}, num_feats:{num_feats}, num_hidden:{num_hidden}, output {outputs},  heads:{heads}, non-linearity:{nonlinearity}, drop:{in_drop}, attn_drop:{attn_drop})')
            model = GAT(g, gnn_layers, num_feats, num_hidden, outputs, heads, nonlinearity, in_drop, attn_drop, residual)
        elif net in ['rgcn']:
            print(f'CREATING RGCN(GRAPH, gnn_layers:{gnn_layers}, num_feats:{num_feats}, num_hidden:{num_hidden}, num_rels:{num_rels}, non-linearity:{nonlinearity}, drop:{in_drop})')
            model = RGCN(g, gnn_layers, num_feats, num_hidden, num_rels, nonlinearity, in_drop, num_bases=num_bases)
        else:
            print('No valid GNN model specified')
            sys.exit(0)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    if previous_model is not None:
        model.load_state_dict(torch.load(previous_model, map_location=device))

    model = model.to(device)

    for epoch in range(epochs):
        if stop_training:
            print("Stopping training. Please wait.")
            break
        model.train()
        loss_list = []
        for batch, data in enumerate(train_dataloader):
            subgraph, feats, labels = data
            subgraph.set_n_initializer(dgl.init.zero_initializer)
            subgraph.set_e_initializer(dgl.init.zero_initializer)
            feats = feats.to(device)
            labels = labels.to(device)
            if fw == 'dgl':
                model.g = subgraph
                for layer in model.layers:
                    layer.g = subgraph
                if net in ['rgcn']:
                    logits = model(feats.float(), subgraph.edata['rel_type'].squeeze().to(device))
                else:
                    logits = model(feats.float())
            loss = loss_fcn(logits[getMaskForBatch(subgraph)], labels.float())
            optimizer.zero_grad()
            a = list(model.parameters())[0].clone()
            loss.backward()
            optimizer.step()
            b = list(model.parameters())[0].clone()
            not_learning = torch.equal(a.data, b.data)
            if not_learning:
                print('Not learning')
                sys.exit(1)
            loss_list.append(loss.item())
        loss_data = np.array(loss_list).mean()
        if loss_data < min_train_loss:
            min_train_loss = loss_data
        print('Loss: {}'.format(loss_data))
        output_list_records_train_loss.append(float(loss_data))
        if epoch % 5 == 0:
            print("Epoch {:05d} | Loss: {:.6f} | Patience: {} | ".format(epoch, loss_data, cur_step), end='')
            score_list = []
            val_loss_list = []
            for batch, valid_data in enumerate(valid_dataloader):
                subgraph, feats, labels = valid_data
                subgraph.set_n_initializer(dgl.init.zero_initializer)
                subgraph.set_e_initializer(dgl.init.zero_initializer)
                feats = feats.to(device)
                labels = labels.to(device)
                score, val_loss = evaluate(feats.float(), model, subgraph, labels.float(), loss_fcn, fw, net)
                score_list.append(score)
                val_loss_list.append(val_loss)
            mean_score = np.array(score_list).mean()
            mean_val_loss = np.array(val_loss_list).mean()
            print("Score: {:.6f} MEAN: {:.6f} BEST: {:.6f}".format(mean_score, mean_val_loss, best_loss))
            output_list_records_dev_loss.append(mean_val_loss)

            # early stop
            if best_loss > mean_val_loss or best_loss < 0:
                print('Saving...')
                directory = str(identifier).zfill(5)
                os.system('mkdir ' + directory + ' 2> /dev/null')
                best_loss = mean_val_loss
                if best_loss < min_dev_loss:
                    min_dev_loss = best_loss

                # Save the model
                model.eval()
                torch.save(model.state_dict(), directory+'/calibration.tch')
                params = {
                    'train_loss': min_train_loss,
                    'dev_loss': min_dev_loss,
                    'net': net,
                    'fw': fw,
                    'num_gnn_layers': gnn_layers,
                    'num_feats': num_feats,
                    'num_hidden': num_hidden,
                    'graph_type': graph_type,
                    'heads': heads,
                    'non-linearity': nonlinearity_str,
                    'in_drop': in_drop,
                    'attn_drop': attn_drop,
                    'alpha': alpha,
                    'residual': residual,
                    'num_rels': num_rels,
                    'num_bases': num_bases,
                    'train_scores': output_list_records_train_loss,
                    'dev_scores': output_list_records_dev_loss
                    }
                pickle.dump(params, open(directory+'/calibration.prms', 'wb'))
                cur_step = 0
            else:
                cur_step += 1
                if cur_step >= patience:
                    break
    time_a = time.time()
    test_score_list = []
    for batch, test_data in enumerate(test_dataloader):
        subgraph, feats, labels = test_data
        subgraph.set_n_initializer(dgl.init.zero_initializer)
        subgraph.set_e_initializer(dgl.init.zero_initializer)
        feats = feats.to(device)
        labels = labels.to(device)
        test_score_list.append(evaluate(feats, model, subgraph, labels.float(), loss_fcn, fw, net)[1])
    time_b = time.time()
    time_delta = float(time_b-time_a)
    test_loss = np.array(test_score_list).mean()
    print("MSE for the test set {}".format(test_loss))
    return min_train_loss, min_dev_loss, test_loss, time_delta, num_of_params(model), epoch, output_list_records_train_loss, output_list_records_dev_loss


if __name__ == '__main__':
    list_of_tasks = pickle.load(open('LIST_OF_TASKS.pckl', 'rb'))
    
    for tttxxx in range(1):
        index = random.randrange(start=0, stop=len(list_of_tasks))
        gone = 0
        while list_of_tasks[index]['train_loss'] >= 0:
            index += 1
            if index >= len(list_of_tasks):
                index = 0
                gone += 1
                if gone > 2:
                    print("Looped twice, that means we are done.")
                    sys.exit(0)
        print('GOT THE FOLLOWING TASK FROM THE LIST:', list_of_tasks[index])
        list_of_tasks[index]['train_loss'] = 0
        pickle.dump(list_of_tasks, open('LIST_OF_TASKS.pckl', 'wb'))
        task = list_of_tasks[index]


        time_a = time.time()
        train_loss, dev_loss, test_loss, test_time, num_parameters, last_epoch, train_scores, dev_scores = main(
            'new_dataset/new_human_data_training.json',
            'merged_human_data_test_square_i_real.json',
            'merged_human_data_test_inf.json',
            task)
        time_b = time.time()
    
        list_of_tasks[index]['train_loss'] = train_loss
        list_of_tasks[index]['dev_loss'] = dev_loss
        list_of_tasks[index]['test_loss'] = test_loss
        list_of_tasks[index]['test_time'] = test_time
        list_of_tasks[index]['num_parameters'] = num_parameters
        list_of_tasks[index]['elapsed'] = time_b - time_a
        list_of_tasks[index]['last_epoch'] = last_epoch
        list_of_tasks[index]['train_scores'] = train_scores
        list_of_tasks[index]['dev_scores'] = dev_scores
        pickle.dump(list_of_tasks, open('LIST_OF_TASKS.pckl', 'wb'))


